{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fe5c2c-bb84-4a19-85e2-bb6a56b866eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import requests\n",
    "import json\n",
    "from chromadb.api.types import EmbeddingFunction\n",
    "\n",
    "# Crear la función de embeddings usando Ollama\n",
    "class OllamaEmbeddingFunction(EmbeddingFunction):\n",
    "    def __call__(self, input):\n",
    "        embeddings = []\n",
    "        for text in input:\n",
    "            response = requests.post(\n",
    "                \"http://localhost:11434/api/embeddings\",  # Endpoint correcto para embeddings\n",
    "                json={\"model\": \"nomic-embed-text\", \"prompt\": text}  # Enviar un solo texto a la vez\n",
    "            )\n",
    "            response_json = response.json()\n",
    "            \n",
    "            if \"embedding\" in response_json:\n",
    "                embeddings.append(response_json[\"embedding\"])  # Guardar el embedding\n",
    "            else:\n",
    "                raise ValueError(f\"Error en la respuesta de Ollama: {response_json}\")\n",
    "        \n",
    "        return embeddings  # Retorna una lista de embeddings\n",
    "\n",
    "# Instanciar la función de embeddings con Ollama\n",
    "embedding_function = OllamaEmbeddingFunction()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031a123f-be36-41bd-b74e-d2dbe70e5a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear conexión con ChromaDB\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "# Crear la colección con Ollama como generador de embeddings\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"markdown_collection\",\n",
    "    embedding_function=embedding_function  # Pasamos la función de embeddings personalizada\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a15f120-b0d5-4377-a0fa-b846e4773e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def load_markdown_file(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def chunk_text(text, chunk_size=300):\n",
    "    sentences = re.split(r'\\n\\n+', text)  # Separar por párrafos (mejor para Markdown)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        current_length += len(sentence)\n",
    "        current_chunk.append(sentence)\n",
    "\n",
    "        if current_length >= chunk_size:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd359a8-373e-4af3-869d-fcaf3cfa7b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer el archivo Markdown\n",
    "file_path = \"/Users/jiabowang/Desktop/dgsi/DGSI/reto2/pagina.md\"\n",
    "text = load_markdown_file(file_path)\n",
    "\n",
    "# Dividir el texto en chunks\n",
    "chunks = chunk_text(text, chunk_size=300)\n",
    "\n",
    "# Insertar los chunks en ChromaDB\n",
    "collection.upsert(\n",
    "    documents=chunks,\n",
    "    ids=[f\"chunk_{i}\" for i in range(len(chunks))]\n",
    ")\n",
    "\n",
    "print(f\"Se han insertado {len(chunks)} chunks en ChromaDB.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a1363a-e4bb-4cfe-9bb9-e4708a86b658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "result = ' '.join(chunks) \n",
    "preguntas = input(\"Itroduce tus preguntas\")\n",
    "prompt = f\"\"\"Eres un asistente experto en responder preguntas basándote en información fragmentada en múltiples chunks almacenados en ChromaDB. \n",
    "A continuación, te proporcionaré un fragmento de información que necesitas aprender para usar en el futuro para responder preguntas relacionadas. \n",
    "El fragmento de información es el siguiente:\n",
    "{chunks}\n",
    "Por favor, recuerda este fragmento de información y utilízalo cuando te realicen preguntas sobre este contenido.\n",
    "\n",
    "Para cada pregunta, sigue estos pasos:\n",
    "Recuperación: Busca en ChromaDB los chunks más relevantes relacionados con la consulta.\n",
    "Síntesis: Analiza la información recuperada y combínala en una respuesta coherente y clara.\n",
    "Explicación: Si la información provista es incompleta o ambigua, indícalo y sugiere posibles interpretaciones o fuentes adicionales.\n",
    "Formato: Responde en un estilo natural y conciso, asegurando que la información fluya de manera lógica.\n",
    "\n",
    "Ejemplo de respuesta:\n",
    "Pregunta: \"¿Cuál es la capital de Francia?\"\n",
    "Chunks recuperados: [\"París es la ciudad más poblada de Francia y su centro político.\", \"Francia tiene su capital en París desde hace siglos.\"]\n",
    "Respuesta generada: \"La capital de Francia es París, que también es su ciudad más poblada y centro político.\"\n",
    "\n",
    "Si no encuentras información relevante en los chunks almacenados, responde con:\n",
    "\"No encontré información suficiente en la base de datos para responder con certeza. ¿Quieres que intente reformular la búsqueda o consulte otra fuente?\"\n",
    "Mi pregunta es: {pregunta}\"\"\"\n",
    "\n",
    "def interactuar_open_ai(prompt):\n",
    "    openai.api_key = \"api-key\"\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.3,\n",
    "    )\n",
    "\n",
    "    # Extract the simplified text from the API response\n",
    "    simplified_text = response.choices[0].message.content\n",
    "    return simplified_text\n",
    "\n",
    "interactuar_open_ai(prompt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
